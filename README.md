# Multilingual Transformers in Code-Mixed Speech and Text: A Survey Across Core NLP Tasks

# ✨ Highlights
Code-mixing, the practice of alternating between two or more languages within a single utterance or text, is 
increasingly common in multilingual societies, particularly in informal settings such as social media, conversation, and 
user-generated content. This work provides a comprehensive overview of how modern multilingual transformer-based 
models, especially language models (LMs), handle this phenomenon across key NLP tasks.


Code-Mixing Prevalence: Widely observed in social platforms, informal speech, and user content in multilingual 
regions.

Challenges for NLP: Traditional monolingual NLP models face difficulty due to:

* Lexical inconsistencies

* Syntactic variation

* Semantic ambiguity

Focus of the Survey: Reviews transformer-based solutions for handling code-mixed data on core NLP 
tasks. Key challenges, Model adaptations, Evaluation benchmarks, and Limitations of existing models analysed for each of
the following core NLP tasks.

|   **Speech-to-text**    | **Speaker identification** |
|:-----------------------:|:--------------------------:|
| **Text Classification** |  **Token Classification**  |



# 🚀 Quick Start
| S.No |  STEP  |                         PROCESS                          | 
|:----:|:------:|:--------------------------------------------------------:|
|  1.  |   🔧   |            [Environment Setup](docs/SETUP.md)            |
|  2.  | 🧹📊🔤 |        [Data Preparation](docs/DOWNLOAD_DATA.md)         |
|  3.  |   🧐   |  [Qualitative Analysis](notebooks/Qualitative-Analysis)  |
|  4.  |  📈📉  | [Quantitative Analysis](notebooks/Quantitative-Analysis) |


# ⚙️ OS Compatibility Validated

| Platform  | Supported |
|-----------|:---------:|
| 🪟 Windows | ❌        |
| 🍎 macOS   | ❌        |
| 🐧 Linux   | ✅        |


# 📖 Citation
```
Barathi Ganesh HB and Ptaszynski Michal. 2025. Multilingual Transformers in Code-Mixed Speech and Text:
A Survey Across Core NLP Tasks. J. ACM XX, X, Article XXX (June 2025), 10 pages. https://doi.org/XXXXXXX.
XXXXXXX
```
